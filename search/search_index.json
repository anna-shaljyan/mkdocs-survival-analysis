{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#contributors","title":"Contributors:","text":"<ul> <li>Anna Shaljyan</li> <li>Anna Manasyan</li> <li>Ela Khachatryan</li> <li>Sergey Tovmasyan</li> </ul>"},{"location":"model/","title":"Model of the project","text":""},{"location":"model/#code-of-the-model","title":"Code of the model","text":"model_AFT.py<pre><code>import pandas as pd\nimport os\nimport logging\nfrom ..logger import CustomFormatter\nfrom lifelines import WeibullAFTFitter, LogNormalAFTFitter, LogLogisticAFTFitter\nfrom lifelines.fitters import ParametricRegressionFitter\nfrom autograd import numpy as np\n\n# Initialize and configure the logger\nlogger = logging.getLogger(os.path.basename(__file__))\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nch.setFormatter(CustomFormatter())\nlogger.addHandler(ch)\n\n\nclass ExponentialAFTFitter(ParametricRegressionFitter):\n    '''\n    This is a class for implementing an Exponential AFT Fitter Model.\n    '''\n    # this class property is necessary, and should always be a non-empty list of strings.\n    _fitted_parameter_names = ['lambda_']\n\n    def _cumulative_hazard(self, params, t, Xs):\n        # params is a dictionary that maps unknown parameters to a numpy vector.\n        # Xs is a dictionary that maps unknown parameters to a numpy 2d array\n        beta = params['lambda_']\n        X = Xs['lambda_']\n        lambda_ = np.exp(np.dot(X, beta))\n        return t / lambda_\n\nclass AFTModelSelector:\n    \"\"\"\n    A class for selecting the best AFT (Accelerated Failure Time) model among Weibull, Exponential,\n    Log-Normal, and Log-Logistic models based on AIC, and generating churn rate and customer lifetime value (CLV) \n    predictions for a specified number of time periods.\n\n    Parameters:\n    - data (pd.DataFrame): The input DataFrame containing survival data.\n    - primary_col(str): The column name in the DataFrame representing the primary key.\n    - duration_col (str): The column name in the DataFrame representing the duration or time-to-event.\n    - event_col (str): The column name in the DataFrame representing the event indicator.\n\n    Attributes:\n    - data (pd.DataFrame): The input DataFrame containing survival data.\n    - primary(str): The column name in the DataFrame representing the primary key.\n    - duration_col (str): The column name in the DataFrame representing the duration or time-to-event.\n    - event_col (str): The column name in the DataFrame representing the event indicator.\n    - aft_model (lifelines.Fitter): The selected AFT model based on AIC.\n    - predictions_df (pd.DataFrame): DataFrame containing churn and CLV predictions for a specified number of time periods.\n    \"\"\"\n\n    def __init__(self, data: pd.DataFrame , primary_col:str,  duration_col : str, event_col: str):\n        self.data = data\n        self.primary = primary_col\n        self.duration_col = duration_col\n        self.event_col = event_col\n        self.aft_model = None\n        self.predictions_df = None\n\n\n\n    def select_best_model(self):\n        \"\"\"\n        Selects the best AFT model among Weibull, Exponential, Log-Normal, and Log-Logistic models based on AIC.\n        Stores the selected model in the 'aft_model' attribute.\n        \"\"\"\n        models = {\n            'Weibull': WeibullAFTFitter(),\n            'Exponential': ExponentialAFTFitter(),\n            'LogNormal': LogNormalAFTFitter(),\n            'LogLogistic': LogLogisticAFTFitter(),\n        }\n\n        best_aic = float('inf')\n        best_model = None\n\n        # Handle zero values in the duration column\n        self.data[self.duration_col] = self.data[self.duration_col].replace(0, 0.0001)\n\n        for model_name, model in models.items():\n            model.fit(self.data, duration_col= self.duration_col, event_col= self.event_col)\n\n            aic = model.AIC_\n            logger.info(f\"{model_name} AIC: {aic}\")\n\n            if aic &lt; best_aic:\n                best_aic = aic\n                best_model = model_name\n\n        logger.warning(f\"\\nBest Model: {best_model} with AIC: {best_aic}\")\n        self.aft_model = models[best_model]\n\n\n    def fit_and_predict(self, n_time_periods: int):\n        \"\"\"\n        Fits the selected AFT model and generates churn predictions for a specified number of time periods.\n        Stores the predictions in the 'predictions_df' attribute.\n\n        Parameters:\n        - n_time_periods (int): The number of time periods for which predictions should be generated.\n\n        Returns:\n        - str: A message indicating the model ran successfully. \n        \"\"\"\n        if self.aft_model is None:\n            logger.warning(\"Please run select_best_model() first.\")\n            return\n\n        # Handle zero values in the duration column\n        self.data[self.duration_col] = self.data[self.duration_col].replace(0, 0.0001)\n\n        predictions_df_list = []\n\n        for time_period in range(1, n_time_periods + 1):\n            customer_data = pd.DataFrame({\n                'customer_id': self.data[self.primary],\n                'pred_period': time_period\n            })\n\n            # Generate survival predictions \n            predictions = self.aft_model.predict_survival_function(self.data, times=[time_period])\n\n            #obtaining churn predictions\n            churn = round(1 - predictions, 5)\n            # Convert predictions to a DataFrame\n            predictions_df = pd.DataFrame(churn.T.values, columns=['churn_rate'])\n\n            # Concatenate customer_id and time_period with predictions DataFrame\n            result_df = pd.concat([customer_data, predictions_df], axis=1)\n\n            # Append to the list\n            predictions_df_list.append(result_df)\n\n        # Concatenate all predictions into a single DataFrame\n        self.predictions_df = pd.concat(predictions_df_list, ignore_index=True)\n        logger.info(\"The AFT model was run successfully.\")\n\n    def calculate_clv(self, MM=1300, r=0.1):\n        \"\"\"\n        Calculates Customer Lifetime Value (CLV) for each customer in 'predictions_df' attribute \n        and updates the dataframe to include CLV predictions.\n\n        Parameters:\n        - MM (float): A constant representing the monetary value.\n        - r (float): The periodic interest rate for discounting.\n\n        Returns:\n        - str: A message indicating the 'predictions_df' attribute was updated successfully. \n        \"\"\"\n\n        predictions_dfs = []  # List to store individual CLV prediction DataFrames\n        if self.predictions_df is None:\n            logger.warning(\"Please run fit_and_predict() first.\")\n            return\n\n        #Making the data long format\n        data_clv = self.predictions_df.pivot(index='customer_id', columns='pred_period', values='churn_rate')\n        #Calculating the Survival Rates from Churn Rates \n        data_clv = 1 - data_clv\n\n        # Iterating over an increasing number of columns\n        for i in range(1, len(data_clv.columns) + 1):\n            # Selecting the first i columns\n            subset_data = data_clv.iloc[:, :i]\n\n            # Calculating CLV \n            data_clv1 = subset_data.copy()\n            sequence = list(range(1, len(data_clv1.columns) + 1))\n\n            for num in sequence:\n                data_clv1.iloc[:, num - 1] /= (1 + r/12) ** (num - 1)\n\n            data_clv1['CLV'] = MM * data_clv1.sum(axis=1)\n\n            predictions_dfs.append(data_clv1['CLV'])\n\n        clv_prediction = pd.concat(predictions_dfs, axis=1)\n        clv_prediction.columns = range(1, len(predictions_dfs) + 1)\n\n        #returning to original data format and saving the predictions\n        clv_prediction = clv_prediction.reset_index()\n        clv_prediction = pd.melt(clv_prediction, id_vars=['customer_id'], var_name='pred_period', value_name='CLV')\n\n        #Combining the results and updating the predictions dataframe\n        self.predictions_df = pd.merge(self.predictions_df, clv_prediction, on=['customer_id','pred_period'], how='left')\n        logger.info(\"The CLV predictions were added successfully.\")\n</code></pre>"},{"location":"api_guide/api_usage/","title":"API","text":""},{"location":"api_guide/api_usage/#how-to-activate-apipy","title":"How to activate api.py","text":"<p>Run <code>run.py</code> to see initially a message in port, add /docs to see put methods and two get endpoints besides message. Port should look something like this: http://127.0.0.1:8000/docs#/ . You can run <code>run.py</code> by executing python run.py in your terminal in venv. </p> <pre><code>python run.py\n</code></pre> run.py<pre><code>#Importing libraries\nimport uvicorn\nimport os\nimport fastapi\n\nfrom survival_analysis.api import app\n\nif __name__== \"__main__\":\n    uvicorn.run(app)\n</code></pre>"},{"location":"api_guide/api_usage/#code","title":"Code","text":"api.py<pre><code>from fastapi import FastAPI, HTTPException, Query, File, UploadFile, Path\nfrom fastapi.responses import JSONResponse\nfrom fastapi.encoders import jsonable_encoder\nfrom datetime import datetime\nimport sqlite3\nimport logging\nfrom ..logger import CustomFormatter\nfrom ..database_preparation import SqlHandler\nimport os\nimport pandas as pd\nfrom typing import Any, List, Union, Optional\nimport traceback\n\napp = FastAPI()\n\nlogger = logging.getLogger(os.path.basename(__file__))\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nch.setFormatter(CustomFormatter())\nlogger.addHandler(ch)\n\n# Creating instances of SqlHandler for each table\ndim_customer_handler = SqlHandler(dbname='sa_db', table_name='DimCustomer')\nfact_predictions_handler = SqlHandler(dbname='sa_db', table_name='FactPredictions')\nfact_push_notification_handler = SqlHandler(dbname='sa_db', table_name='FactPushNotification')\nfact_email_handler = SqlHandler(dbname='sa_db', table_name='FactEmail')\n\n# Defining functions to open connections to the databases\ndef get_dim_customer_db():\n    return dim_customer_handler.cnxn\n\ndef get_fact_predictions_db():\n    return fact_predictions_handler.cnxn\n\ndef get_fact_push_notification_db():\n    return fact_push_notification_handler.cnxn\n\ndef get_fact_email_db():\n    return fact_email_handler.cnxn\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    # Closing the database connections on shutdown\n    dim_customer_handler.close_cnxn()\n    fact_predictions_handler.close_cnxn()\n    fact_push_notification_handler.close_cnxn()\n    fact_email_handler.close_cnxn()\n\n@app.get(\"/\")\nasync def root():\n    return {\n        \"\"\"\n            Initializing API for Survival Analysis project with selecting data from database, inserting data, and updating data. It now contains also several endpoints that help to identify top customers with the highest churn rate, customers with the highest/lowest CLV. Add /docs to see available endpoints.\n        \"\"\"\n    }\n\ndef read_csv(file: UploadFile = File(...)):\n    content = pd.read_csv(file.file)\n    return content\n\ndef validate_sent_date(sent_date):\n    try:\n        # Trying to parse the send_date as a datetime object\n        datetime.strptime(sent_date, '%d/%m/%Y')\n        return True\n    except ValueError:\n        return False\n\n@app.put(\"/populate_fact_push_notification\")\nasync def populate_fact_push_notification(file: UploadFile = File(...)):\n    try:\n        # Reading the CSV file\n        data = read_csv(file)\n\n        # Validating sent_date column\n        if 'sent_date' in data.columns and not all(data['sent_date'].apply(validate_sent_date)):\n            raise HTTPException(status_code=400, detail=\"Invalid or missing values in the sent_date column.\")\n\n        # Inserting data into FactPushNotification table\n        fact_push_notification_handler.insert_many(data)\n\n        return JSONResponse(content=jsonable_encoder({\"message\": \"Data loaded successfully\"}), status_code=200)\n\n    except Exception as e:\n        logger.error(f\"Failed to populate FactPushNotification: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Failed to populate FactPushNotification: {str(e)}\")\n\n@app.put(\"/populate_fact_email\")\nasync def populate_fact_email(file: UploadFile = File(...)):\n    try:\n        # Reading the CSV file\n        data = read_csv(file)\n\n        # Validating sent_date column\n        if 'sent_date' in data.columns and not all(data['sent_date'].apply(validate_sent_date)):\n            raise HTTPException(status_code=400, detail=\"Invalid or missing values in the sent_date column.\")\n\n        # Inserting data into FactEmail table\n        fact_email_handler.insert_many(data)\n\n        return JSONResponse(content=jsonable_encoder({\"message\": \"Data loaded successfully\"}), status_code=200)\n\n    except Exception as e:\n        logger.error(f\"Failed to populate FactEmail: {str(e)}\")\n        logger.error(traceback.format_exc())\n        raise HTTPException(status_code=500, detail=f\"Failed to populate FactEmail: {str(e)}\")\n\n\n## Adding endpoints for different scenarios\n\n@app.get(\"/get_top_churn_clv_customers\")\nasync def get_top_churn_clv_customers(\n    pred_period: int = Query(..., description=\"Prediction period (1-12)\"),\n    top_percentage: int = Query(10, description=\"Top percentage of customers to select (e.g., 10 for top 10%)\", ge=1, le=100)\n):\n    try:\n        handler = fact_predictions_handler\n\n        # Calling the from_sql_to_pandas function to get the data for FactPredictions\n        fact_data = handler.from_sql_to_pandas(chunksize=1000, id_value='customer_ID')\n\n        # Filtering the data for the specified prediction period\n        selected_data = fact_data[fact_data['pred_period'] == pred_period]\n\n        # Sorting the data first by Churn_Rate and then by CLV in descending order\n        sorted_data = selected_data.sort_values(by=['Churn_Rate', 'CLV'], ascending=[False, False])\n\n        # Calculating the number of customers to select based on the specified top percentage\n        total_customers = len(sorted_data)\n        top_count = int(top_percentage / 100 * total_customers)\n\n        # Selecting the top percentage of customers with the highest churn rate and then CLV\n        top_churn_customers = sorted_data.head(top_count)\n\n        if top_churn_customers.empty:\n            return {\"message\": \"No customers found\"}\n\n        # Returning pred_period, customer_id, churn_rate, and clv without merging\n        result_data = top_churn_customers[['pred_period', 'customer_ID', 'Churn_Rate', 'CLV']].to_dict(orient='records')\n\n        return result_data\n    except Exception as e:\n        logger.error(f\"Failed to get top churn customers: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Failed to get top churn customers: {str(e)}\")\n\n\n@app.get(\"/get_top_clv_customers\")\nasync def get_top_clv_customers(\n    top_percentage: int = Query(..., description=\"Percentage of customers to select (e.g., 20 for top 20%)\", ge=1, le=100),\n    pred_period: int = Query(12, description=\"Prediction period (1-12)\")\n):\n    try:\n        handler = fact_predictions_handler\n\n        # Calling the from_sql_to_pandas function to get the data for FactPredictions\n        fact_data = handler.from_sql_to_pandas(chunksize=1000, id_value='customer_ID')\n\n        # Filtering the data for the specified prediction period\n        selected_data = fact_data[fact_data['pred_period'] == pred_period]\n\n        # Sorting the data by CLV in descending order\n        sorted_data = selected_data.sort_values(by='CLV', ascending=False)\n\n        # Calculating the number of customers to select based on the specified top percentage\n        total_customers = len(sorted_data)\n        selected_count = int(top_percentage / 100 * total_customers)\n\n        # Selecting the top percentage of customers\n        selected_customers = sorted_data.head(selected_count)\n\n        if selected_customers.empty:\n            return {\"message\": \"No customers found\"}\n\n        # Returning only customer_id and pred_period without merging\n        result_data = selected_customers[['customer_ID', 'pred_period']].to_dict(orient='records')\n\n        return {\"selected_customers\": result_data}\n    except Exception as e:\n        logger.error(f\"Failed to get top CLV customers: {str(e)}\")\n        raise HTTPException(status_code=500, detail=f\"Failed to get top CLV customers: {str(e)}\")\n</code></pre>"},{"location":"api_guide/api_usage/#api-overview","title":"API Overview","text":""},{"location":"api_guide/api_usage/#use-cases","title":"Use cases","text":""},{"location":"api_guide/api_usage/#the-api-contains","title":"The API contains:","text":""},{"location":"api_guide/api_usage/#1-two-get-methods-get_top_churn_clv_customers-get_top_clv_customers","title":"1. Two get methods: get_top_churn_clv_customers &amp; get_top_clv_customers","text":"<ul> <li>First endpoint /get_top_churn_clv_customers accepts pred_period and number of percentage for sorting customers initially by churn_rate and then by clv. It returns top x% customers based on churn_rate &amp; CLV.</li> </ul>"},{"location":"api_guide/api_usage/#sample-test","title":"Sample test:","text":"<ul> <li>pred_period = 5 </li> <li>top_percentage = 20</li> </ul>"},{"location":"api_guide/api_usage/#result-get_top_churn_clv_customers","title":"Result /get_top_churn_clv_customers","text":"<ul> <li>Second endpoint /get_top_clv_customers accepts pred_period and number of percentage for sorting customers by CLV. It returns top x% customers based on CLV.</li> </ul>"},{"location":"api_guide/api_usage/#sample-test_1","title":"Sample test:","text":"<ul> <li>pred_period = 12</li> <li>top_percentage = 10</li> </ul>"},{"location":"api_guide/api_usage/#result-get_top_clv_customers-accepts","title":"Result /get_top_clv_customers accepts","text":""},{"location":"api_guide/api_usage/#2-two-put-methods-populate_fact_push_notification-populate_fact_email","title":"2. Two put methods: populate_fact_push_notification &amp; populate_fact_email","text":"<p>These two methods are created to populate the DB with the results of actions taken in response to the two get methods mentioned above. There are two csv files email_data.csv and notifications_data.csv in Raw Data folder that contain sample generated data with structure that matches tables of the database.</p> <ul> <li>First put method allows to choose a csv file to add in FactPushNotification table. </li> </ul>"},{"location":"api_guide/api_usage/#excel-file-input-data-rows-for-factpushnotification-table","title":"Excel file input data rows for FactPushNotification table.","text":""},{"location":"api_guide/api_usage/#sample-test-csv-factpushnotification","title":"Sample test csv - FactPushNotification","text":""},{"location":"api_guide/api_usage/#sample-test-csv-result-response-factpushnotification","title":"Sample test csv result response- FactPushNotification","text":""},{"location":"api_guide/api_usage/#sample-test-csv-added-in-table-factpushnotification","title":"Sample test csv added in table- FactPushNotification","text":"<ul> <li>Second put method allows to choose a csv file to add in FactEmail table. </li> </ul>"},{"location":"api_guide/api_usage/#excel-file-input-data-rows-for-factemail-table","title":"Excel file input data rows for FactEmail table.","text":""},{"location":"api_guide/api_usage/#sample-test-csv-factemail","title":"Sample test csv - FactEmail","text":""},{"location":"api_guide/api_usage/#sample-test-csv-result-response-factemail","title":"Sample test csv result response- FactEmail","text":""},{"location":"api_guide/api_usage/#sample-test-csv-added-in-table-factemail","title":"Sample test csv added in table- FactEmail","text":""},{"location":"database_guide/database/","title":"Database","text":""},{"location":"database_guide/database/#erd-structure","title":"ERD Structure","text":""},{"location":"database_guide/database/#overview-of-tables","title":"Overview of Tables","text":""},{"location":"database_guide/database/#dimcustomer-table","title":"DimCustomer Table","text":""},{"location":"database_guide/database/#factpredictions-table","title":"FactPredictions Table","text":""},{"location":"database_guide/database/#factpushnotification-table","title":"FactPushNotification Table","text":""},{"location":"database_guide/database/#factemail-table","title":"FactEmail Table","text":""},{"location":"database_guide/database/#code-parts","title":"Code parts","text":"schema.py<pre><code>\"\"\"\nModule: schema.py\n\nThis module contains Python code for defining and creating a database schema using SQLAlchemy. \nIt defines four tables: 'DimCustomer', 'FactPredictions' 'FactPushNotification' and 'FactEmail'.\n\nIt also configures a custom logger for informational messages regarding the schema creation.\n\n\"\"\"\n\nimport logging\nimport os\nfrom ..logger import CustomFormatter\n\n# Initialize and configure the logger\nlogger = logging.getLogger(os.path.basename(__file__))\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nch.setFormatter(CustomFormatter())\nlogger.addHandler(ch)\n\nfrom sqlalchemy import create_engine, Column, Integer, String, Float, DATE, DateTime, ForeignKey\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, relationship\nfrom datetime import datetime\n\n# Define and configure the database engine (Change the connection URL as needed)\nengine = create_engine('sqlite:///sa_db.db')\n\n# Create a base class for declarative class definitions\nBase = declarative_base()\n\nclass Customer(Base):\n    \"\"\"\n    Class: Customer\n\n    This class defines the 'DimCustomer' table, which represents customer information.\n\n    Attributes:\n    - Customer_ID (int): Primary key for the customer.\n    - Age (int): Customer's age.\n    - Tenure (int): Customer's tenure with the company.\n    - Gender (str): Customer's gender.\n    - Income (int): Customer's income.\n    - Marital_Status (str): Customer's marital status.\n    - Address_ID (int): Customer's address identifier.\n    - Education (str): Customer's education level.\n    - Retirement (str): Customer's retirement status.\n    - Churn (str): Customer's churn status.\n    - Region (str): Customer's region.\n    - Service_Category (str): Service category.\n    - Voice_Included (str): Voice service inclusion status.\n    - Internet_Included (str): Internet service inclusion status.\n    - Forward_Included (str): Forwarding service inclusion status.\n    \"\"\"\n    __tablename__ = \"DimCustomer\"\n\n    Customer_ID = Column(Integer, primary_key=True)\n    Age = Column(Integer)\n    Tenure = Column(Integer)\n    Gender = Column(String(10))\n    Income = Column(Integer)\n    Marital_Status = Column(String(15))\n    Address_ID = Column(Integer)\n    Education = Column(String(45))\n    Retirement = Column(String(3))\n    Churn = Column(String(3))\n    Region = Column(String(15))\n    Service_Category = Column(String(15))\n    Voice_Included = Column(String(3))\n    Internet_Included = Column(String(3))\n    Forward_Included = Column(String(3))\n\nclass FactPredictions(Base):\n    \"\"\"\n    Class: FactPredictions\n\n    This class defines the 'FactPredictions' table, which stores predictive information related to customers.\n\n    Attributes:\n    - pred_period (int): Primary key representing the prediction period.\n    - customer_ID (int): Foreign key referencing the 'DimCustomer' table.\n    - CLV (float): Customer Lifetime Value.\n    - Churn_Rate (float): Churn rate.\n    - customer (relationship): Establishes a relationship with the 'DimCustomer' table.\n\n    \"\"\"\n    __tablename__ = \"FactPredictions\"\n\n    pred_period = Column(Integer, primary_key=True)\n    customer_ID = Column(Integer, ForeignKey('DimCustomer.Customer_ID'), primary_key=True)\n    CLV = Column(Float)\n    Churn_Rate = Column(Float)\n    customer = relationship(\"DimCustomer\")\n\nclass FactPushNotification(Base):\n    \"\"\"\n    Class: FactPushNotification\n\n    This class defines the 'FactPushNotification' table, which stores information about the push notifications sent to customers.\n\n    Attributes:\n    - sent_date (DateTime): Primary key representing the date the push notification was sent.\n    - customer_ID (int): Foreign key referencing the 'DimCustomer' table.\n    - success (int): A binary column representing wether the push notification was sent successfully or not.\n    - customer (relationship): Establishes a relationship with the 'DimCustomer' table.\n\n    \"\"\"\n    __tablename__ = \"FactPushNotification\"\n\n    sent_date = Column(DateTime, primary_key=True)\n    customer_ID = Column(Integer, ForeignKey('DimCustomer.Customer_ID'), primary_key=True)\n    success = Column(Integer)\n    customer = relationship(\"DimCustomer\")\n\nclass FactEmail(Base):\n    \"\"\"\n    Class: FactEmail\n\n    This class defines the 'FactEmail' table, which stores information about the emails sent to customers.\n\n    Attributes:\n    - sent_date (DateTime): Primary key representing the date the push notification was sent.\n    - customer_ID (int): Foreign key referencing the 'DimCustomer' table.\n    - success (int): A binary column representing wether the push notification was sent successfully or not.\n    - customer (relationship): Establishes a relationship with the 'DimCustomer' table.\n\n    \"\"\"\n    __tablename__ = \"FactEmail\"\n\n    sent_date = Column(DateTime, primary_key=True)\n    customer_ID = Column(Integer, ForeignKey('DimCustomer.Customer_ID'), primary_key=True)\n    success = Column(Integer)\n    customer = relationship(\"DimCustomer\")\n\n\n# Create the tables defined in the schema\nBase.metadata.create_all(engine)\n\n# Log a message indicating that the schema has been created\nlogger.info(\"Schema Has Been Created\")\n</code></pre> sql_interactions.py<pre><code>\"\"\"\nModule: sql_interactions.py\n\nThis module defines a Python class called 'SqlHandler' for interacting with SQLite databases. The class allows for various operations on SQLite databases, such as connecting, inserting data, retrieving data, and more.\n\nAttributes:\n- dbname (str): The name of the SQLite database.\n- table_name (str): The name of the table within the database.\n\nMethods:\n\n- __init__(self, dbname: str, table_name: str) -&gt; None:\n    Constructor for the 'SqlHandler' class. Initializes a connection to an SQLite database and specifies the target table.\n\n- close_cnxn(self) -&gt; None:\n    Closes the SQLite database connection.\n\n- insert_one(self, data) -&gt; str:\n    Inserts a single row to the database. \n\n- get_table_columns(self) -&gt; list:\n    Retrieves a list of column names for the specified table.\n\n- truncate_table(self) -&gt; None:\n    Truncates the specified table, removing all its data.\n\n- drop_table(self) -&gt; None:\n    Deletes the specified table from the database.\n\n- insert_many(self, df: pd.DataFrame) -&gt; str:\n    Inserts data from a pandas DataFrame into the specified table, mapping columns to their respective database columns.\n\n- from_sql_to_pandas(self, chunksize: int, id_value: str) -&gt; pd.DataFrame:\n    Retrieves data from the database and loads it into a pandas DataFrame in chunks of a specified size.\n\n- update_table(self, set_dict, cond_dict) -&gt; str:\n    Update rows in a database table based on the set and where conditions provided in dictionaries.\n\n\n\"\"\"\n\nimport sqlite3\nimport logging\nimport pandas as pd\nimport numpy as np\nimport os\nfrom ..logger import CustomFormatter\n\n# Initialize and configure the logger\nlogger = logging.getLogger(os.path.basename(__file__))\nlogger.setLevel(logging.DEBUG)\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\nch.setFormatter(CustomFormatter())\nlogger.addHandler(ch)\n\nclass SqlHandler:\n\n    def __init__(self, dbname: str, table_name: str) -&gt; None:\n        \"\"\"\n        Constructor for the SqlHandler class.\n\n        Parameters:\n        - dbname (str): The name of the SQLite database.\n        - table_name (str): The name of the table within the database.\n        \"\"\"\n        self.cnxn = sqlite3.connect(f'{dbname}.db')\n        self.cursor = self.cnxn.cursor()\n        self.dbname = dbname\n        self.table_name = table_name\n\n    def close_cnxn(self) -&gt; None:\n        \"\"\"\n        Closes the SQLite database connection.\n\n        \"\"\"\n        logger.info('Committing the changes')\n        self.cnxn.close()\n        logger.info('The connection has been closed')\n\n    def insert_one(self, data: dict) -&gt; str:\n        \"\"\"\n        Inserts a single record from a dictionary into the specified table, mapping keys to their respective database columns.\n\n        Parameters:\n        - data (dict): A dictionary containing data to be inserted.\n\n        Returns:\n        - str: A message indicating that the data has been loaded.\n        \"\"\"\n\n        #getting the DB columns\n\n        columns = self.get_table_columns()\n        sql_column_names = [col.lower() for col in columns]\n\n        # Convert keys to lowercase\n        data = {key.lower(): value for key, value in data.items()}\n\n        # Filter the dictionary to keep only keys that match the database column names\n        filtered_data = {key: value for key, value in data.items() if key in sql_column_names}\n\n        # Prepare the values to be inserted\n        row_values = list(filtered_data.values())\n        columns = list(filtered_data.keys())\n\n        ncolumns = ['?' for _ in columns]\n        cols = ', '.join(columns)\n        params = ', '.join(ncolumns)\n\n        logger.info(f'Insert structure: colnames: {cols} params: {params}')\n\n        query = f\"\"\"INSERT INTO {self.table_name} ({cols}) VALUES ({params});\"\"\"\n\n        self.cursor.execute(query, row_values)\n\n        try:\n            logger.info(self.cursor.messages)\n        except:\n            pass\n\n        self.cnxn.commit()\n        logger.warning('The data is loaded')\n\n\n    def get_table_columns(self) -&gt; list:\n        \"\"\"\n        Retrieves a list of column names for the specified table.\n\n        Returns:\n        - column_names (list): List of column names in the table.\n        \"\"\"\n        self.cursor.execute(f\"PRAGMA table_info({self.table_name});\")\n        columns = self.cursor.fetchall()\n        column_names = [col[1] for col in columns]\n        logger.info(f'The list of columns: {column_names}')\n        return column_names\n\n    def truncate_table(self) -&gt; None:\n        \"\"\"\n        Truncates the specified table, removing all its data.\n        \"\"\"\n        query = f\"DELETE FROM {self.table_name};\"\n        self.cursor.execute(query)\n        self.cnxn.commit()\n        logging.info(f'The {self.table_name} is truncated')\n        self.cursor.close()\n\n    def drop_table(self) -&gt; None:\n        \"\"\"\n        Deletes the specified table from the database.\n        \"\"\"\n        query = f\"DROP TABLE IF EXISTS {self.table_name};\"\n        logging.info(query)\n        self.cursor.execute(query)\n        self.cnxn.commit()\n        logging.info(f\"Table '{self.table_name}' deleted.\")\n        logger.debug('Using drop table function')\n\n    def insert_many(self, df: pd.DataFrame) -&gt; str:\n        \"\"\"\n        Inserts data from a pandas DataFrame into the specified table, mapping columns to their respective database columns.\n\n        Parameters:\n        - df (pd.DataFrame): The DataFrame containing data to be inserted.\n\n        Returns:\n        - str: A message indicating that the data has been loaded.\n        \"\"\"\n        df = df.replace(np.nan, None)  # For handling NULLS\n        df.rename(columns=lambda x: x.lower(), inplace=True)\n        columns = list(df.columns)\n        logger.info(f'BEFORE the column intersection: {columns}')\n        sql_column_names = [i.lower() for i in self.get_table_columns()]\n        columns = list(set(columns) &amp; set(sql_column_names))\n        logger.info(f'AFTER the column intersection: {columns}')\n        ncolumns = list(len(columns) * '?')\n        data_to_insert = df.loc[:, columns]\n        values = [tuple(i) for i in data_to_insert.values]\n        logger.info(f'The shape of the table which is going to be imported {data_to_insert.shape}')\n        if len(columns) &gt; 1:\n            cols, params = ', '.join(columns), ', '.join(ncolumns)\n        else:\n            cols, params = columns[0], ncolumns[0]\n        logger.info(f'Insert structure: colnames: {cols} params: {params}')\n        logger.info(values[0])\n        query = f\"\"\"INSERT INTO {self.table_name} ({cols}) VALUES ({params});\"\"\"\n        logger.info(f'QUERY: {query}')\n        self.cursor.executemany(query, values)\n        try:\n            for i in self.cursor.messages:\n                logger.info(i)\n        except:\n            pass\n        self.cnxn.commit()\n        logger.warning('The data is loaded')\n\n    def from_sql_to_pandas(self, chunksize: int, id_value: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Retrieves data from the database and loads it into a pandas DataFrame in chunks of a specified size.\n\n        Parameters:\n        - chunksize (int): The chunksize for data extraction.\n        - id_value (str): The values by which the data should be sorted.\n\n        Returns:\n        - pd.DataFrame: A DataFrame containing the retrieved data.\n        \"\"\"\n        offset = 0\n        dfs = []\n        while True:\n            query = f\"\"\"\n            SELECT * FROM {self.table_name}\n                ORDER BY {id_value} \n                LIMIT {chunksize}\n                OFFSET {offset};\n            \"\"\"\n            data = pd.read_sql_query(query, self.cnxn)\n            logger.info(f'The shape of the chunk: {data.shape}')\n            dfs.append(data)\n            offset += chunksize\n            if len(dfs[-1]) &lt; chunksize:\n                logger.warning('Loading the data from SQL is finished')\n                logger.debug('Connection is closed')\n                break\n        df = pd.concat(dfs)\n        return df\n\n\n    def update_table(self, set_dict: dict, cond_dict: dict) -&gt; str:\n        \"\"\"\n        Update rows in a database table based on the set and where conditions provided in dictionaries.\n\n        Parameters:\n        - set_dict (dict): A dictionary with column names as keys and new values as values for the SET clause.\n        - cond_dict (dict): A dictionary with column names as keys and conditions as values for the WHERE clause.\n\n        Returns:\n        - str: A message indicating that the data has been updated.\n        \"\"\"\n\n        #getting the db column names\n        columns = self.get_table_columns()\n        sql_column_names = [col.lower() for col in columns]\n\n        # Convert keys to lowercase\n        set_dict = {key.lower(): value for key, value in set_dict.items()}\n        cond_dict = {key.lower(): value for key, value in cond_dict.items()}\n\n        # Filter the dictionary to keep only keys that match the database column names\n        set_dict = {key: value for key, value in set_dict.items() if key in sql_column_names}\n        cond_dict = {key: value for key, value in cond_dict.items() if key in sql_column_names}    \n\n        try:\n            # Generate the SET clause\n            set_clause = ', '.join([f\"{col} = ?\" for col in set_dict.keys()])\n            set_values = list(set_dict.values())\n            logger.info(f'Set Clause Structure: {set_clause}')\n\n            # Generate the WHERE clause\n            where_clause = ' AND '.join([f\"{col} = ?\" for col in cond_dict.keys()])\n            where_values = list(cond_dict.values())  # Add values for the WHERE clause\n            logger.info(f'Where Clause Structure: {where_clause}')\n\n            # Build the SQL query\n            query = f\"\"\"\n                UPDATE {self.table_name}\n                SET {set_clause}\n                WHERE {where_clause};\n                    \"\"\"\n            logger.info(f'Generated SQL query: {query}')  # Log the generated query\n            self.cursor.execute(query, set_values + where_values)  # Combine SET and WHERE values\n\n            self.cnxn.commit()\n            logger.warning(f'The table {self.table_name} is updated.')\n        except Exception as e:\n            logger.warning(f\"Error updating rows: {e}\")\n</code></pre>"},{"location":"survival_analysis_package/homepage/","title":"HomePage","text":""},{"location":"survival_analysis_package/homepage/#objectives-and-outcomes-of-survival-analysis-package","title":"Objectives and Outcomes of Survival Analysis Package:","text":"<p>Our easy-to-use package provides highly accurate predictions for companies. By leveraging survival analysis, churn rate prediction, and CLV estimation, this package will empower businesses to make informed, profitable, and datadriven decisions. The predictions generated will offer insights into the company's future performance, enabling businesses to plan for the short-term and long-term with confidence.</p> <p></p>"},{"location":"survival_analysis_package/homepage/#problem-statement","title":"Problem Statement:","text":"<p>In the modern business landscape, companies across diverse sectors struggle with shared challenges. These include customer churn, revenue instability, resource allocation complexities, pricing optimization, and the imperative for personalized customer experiences. Customer churn, signifying the loss of clientele over time, threatens the business\u2019s continuity. Revenue volatility, shaped by dynamic markets and shifting consumer behaviors, introduces financial uncertainties. Personalizing customer experiences has become a fundamental requirement, necessitating a deep understanding of individual preferences and behaviors. These are just a few examples of the many challenges companies face in today's business world, so innovative solutions to address these pressing issues are needed. Understanding customer behavior, reducing churn, and maximizing the lifetime value of customers may affect these problems in a positive way. Our project aims to build a comprehensive model that conducts survival analysis for companies, predicts churn rates, and estimates customer lifetime value (CLV).</p>"},{"location":"survival_analysis_package/homepage/#users-of-the-project","title":"Users of the project:","text":"<p>The target audience for this project includes a wide range of businesses, such as subscription services, e-commerce, finance, banking, insurance, and more, looking to analyze customer behavior and make data-driven decisions. These businesses seek a solution to understand and mitigate churn, improve revenue stability, allocate resources optimally, and enhance customer personalization.</p> <p></p>"},{"location":"survival_analysis_package/homepage/#constraintslimitations","title":"Constraints/limitations:","text":"<p>Selection Bias: Our team will be aware of potential selection bias in the data and take measures to address it. Software Development Challenges: Our project will tackle software development challenges, including package building and API development, to create a user-friendly and accessible solution.</p>"},{"location":"survival_analysis_package/homepage/#data-and-features","title":"Data and Features:","text":"<p>The project will utilize specific data suitable for survival analysis. Feature selection and the choice of a suitable algorithms will be key considerations in our analysis.</p>"},{"location":"survival_analysis_package/homepage/#solution","title":"Solution:","text":"<p>The project's solution will involve the development of an integrated package that conducts survival analysis, predicts churn rates, and estimates CLV. This package will provide businesses with the insights needed to understand where their company will be in the future, enabling them to make the best and most profitable data-driven decisions. To facilitate the storage of appropriate data and ensure easy accessibility, the project will integrate databases for data storage. Additionally, API endpoints will be implemented to streamline usage, allowing businesses to seamlessly integrate the package into their workflows. The package will be designed for ease of use and will address the complex challenges related to customer behavior and decision-making. By addressing these challenges and providing a robust predictive solution, this project aims to empower businesses from various sectors to proactively manage customer retention, revenue, resource allocation, and pricing strategies while improving the overall customer experience</p>"},{"location":"user_guide/userguide/","title":"User Guide to download package","text":""},{"location":"user_guide/userguide/#overview","title":"Overview","text":"<p>The Survival Analysis package is a Python toolkit for analyzing and predicting customer churn and lifetime value using survival analysis techniques. This package encompasses several modules that cover database schema creation, SQL interactions, predictive modeling, and utility functions for data preprocessing.</p>"},{"location":"user_guide/userguide/#installation","title":"Installation","text":"<p><pre><code>pip install survival-analysis\n</code></pre> You can access our package via PyPi using this link: https://pypi.org/project/survival-analysis/0.0.1/</p>"},{"location":"user_guide/userguide/#modules","title":"Modules","text":""},{"location":"user_guide/userguide/#1-schemapy","title":"1. <code>schema.py</code>","text":""},{"location":"user_guide/userguide/#module-description","title":"Module Description:","text":"<p>This module, <code>schema.py</code>, contains Python code for defining and creating a database schema using SQLAlchemy. It defines tables such as 'DimCustomer', 'FactPredictions', 'FactPushNotification', and 'FactEmail' for storing customer information, predictive data, push notification details, and email information, respectively.</p> <pre><code>from survival_analysis import schema\n</code></pre> <p>The obtained database has the below structure: </p>"},{"location":"user_guide/userguide/#2-sql_interactionspy","title":"2. sql_interactions.py","text":""},{"location":"user_guide/userguide/#module-description_1","title":"Module Description:","text":"<p>The sql_interactions module provides a Python class named SqlHandler for interacting with SQLite databases. This class allows various operations such as connecting, inserting data, retrieving data, truncating tables, dropping tables, updating tables, and more.</p> <pre><code>from survival_analysis import sql_interactions\n</code></pre>"},{"location":"user_guide/userguide/#3-model_aftpy","title":"3. model_AFT.py","text":""},{"location":"user_guide/userguide/#module-description_2","title":"Module Description:","text":"<p>The model_AFT module implements an Accelerated Failure Time (AFT) model for predicting customer churn and lifetime value. It includes classes for different AFT models, a model selector for choosing the best model based on AIC, and methods for fitting the model and generating predictions.</p> <pre><code>from survival_analysis import model_AFT\n</code></pre>"},{"location":"user_guide/userguide/#4-utilspy","title":"4. utils.py","text":""},{"location":"user_guide/userguide/#module-description_3","title":"Module Description:","text":"<p>The utils module contains utility functions, including format_dataframe, which converts categorical variables to binary columns using one-hot encoding and ensures correct data types for numeric variables.</p> <pre><code>from survival_analysis import utils\n</code></pre>"},{"location":"user_guide/userguide/#example-usage","title":"Example Usage","text":"<p>An example demonstrating the use of this package can be found at https://github.com/ella-2002e/MA-SurvivalAnalysis-Project/blob/main/Example.ipynb</p>"},{"location":"user_guide/userguide/#api","title":"API","text":"<p>The API extends our Survival Analysis project with functionality to select data from the database and insert data. The API now includes several endpoints that help identify top customers with the highest churn rate, customers with the highest/lowest CLV, etc.</p>"},{"location":"user_guide/userguide/#usage","title":"Usage","text":"<p>Run <code>run.py</code> to see initially a message in port, add /docs to see put methods and two get endpoints besides message. Port should look something like this: http://127.0.0.1:8000/docs#/ . You can run <code>run.py</code> by executing python run.py in your terminal in venv. </p>"},{"location":"user_guide/userguide/#endpoints","title":"ENDPOINTS","text":""},{"location":"user_guide/userguide/#get","title":"GET","text":""},{"location":"user_guide/userguide/#1-get_top_churn_clv_customers","title":"1. get_top_churn_clv_customers","text":"<ul> <li>Accepts pred_period and number of percentage for sorting customers initially by churn_rate and then by clv. It returns top x% customers based on churn_rate &amp; CLV.</li> </ul>"},{"location":"user_guide/userguide/#2-get_top_clv_customers","title":"2. get_top_clv_customers","text":"<ul> <li>Accepts pred_period and number of percentage for sorting customers by CLV. It returns top x% customers based on CLV.</li> </ul>"},{"location":"user_guide/userguide/#put","title":"PUT","text":"<p>These below PUT methods are created to populate the DB with the results of actions taken in response to the two GET methods mentioned above. There are two csv files email_data.csv and notifications_data.csv in Raw Data folder that contain sample generated data with structure that matches tables of the database.</p>"},{"location":"user_guide/userguide/#1-populate_fact_push_notification","title":"1. populate_fact_push_notification","text":"<ul> <li>Allows to choose a csv file to add in FactPushNotification table. Use that method with notifications_data.csv to populate the FactPushNotification table in the DB. Note that, customer_id-s are taken from endpoint: http://127.0.0.1:8000/get_top_churn_clv_customers?pred_period=12&amp;top_percentage=10 </li> </ul>"},{"location":"user_guide/userguide/#2-populate_fact_email","title":"2. populate_fact_email","text":"<ul> <li>Similarly, use email_data.csv to populate FactEmail table with second put method. Note that, email_data customer_id-s are taken from endpoint: http://127.0.0.1:8000/get_top_clv_customers?top_percentage=20&amp;pred_period=5</li> </ul>"},{"location":"user_guide/userguide/#license","title":"License","text":"<p>This package is provided under the MIT License. Feel free to use and modify it in your projects.</p>"}]}